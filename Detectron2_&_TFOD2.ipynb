{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Detectron2 and how does it differ from previous object detection\n",
        "frameworks?\n",
        "\n",
        "ans :\n",
        "\n",
        "## What is Detectron2?\n",
        "\n",
        "Detectron2 is an open-source computer vision framework developed by Facebook AI Research (FAIR). It provides state-of-the-art implementations of modern vision models for various tasks, including:\n",
        "\n",
        "- **Object Detection**: Faster R-CNN, RetinaNet  \n",
        "- **Instance Segmentation**: Mask R-CNN  \n",
        "- **Semantic Segmentation**: DeepLab, Panoptic FPN  \n",
        "- **Keypoint Detection**: Human pose estimation  \n",
        "\n",
        "Due to its clean design, strong performance, and flexibility, Detectron2 is widely used in research, industry applications, and academic projects.\n",
        "\n",
        "---\n",
        "\n",
        "## How Detectron2 Differs from Previous Object Detection Frameworks\n",
        "\n",
        "### 1. Built on PyTorch (Dynamic Graphs)\n",
        "Detectron2 is built using PyTorch, unlike Detectron (v1), which was based on Caffe2. PyTorch’s dynamic computation graphs make debugging easier, allow flexible model customization, and enable faster research experimentation.\n",
        "\n",
        "### 2. Highly Modular and Extensible Design\n",
        "Detectron2 follows a modular design where components such as backbones, heads, datasets, loss functions, and training pipelines are independent. This allows researchers to easily swap architectures, add new datasets, and implement custom losses or heads, which was difficult in earlier frameworks.\n",
        "\n",
        "### 3. Cleaner and More Readable Codebase\n",
        "The framework provides well-structured APIs, better documentation, and consistent coding practices. Older frameworks such as Darknet and early TensorFlow pipelines were harder to maintain and extend.\n",
        "\n",
        "### 4. State-of-the-Art Performance\n",
        "Detectron2 includes optimized implementations of modern architectures and supports mixed precision training, multi-GPU training, and distributed learning. This results in faster training and inference compared to many earlier frameworks.\n",
        "\n",
        "### 5. Unified Support for Multiple Vision Tasks\n",
        "Detectron2 supports object detection, instance segmentation, semantic segmentation, and panoptic segmentation within a single unified framework, whereas earlier frameworks often focused on only one task.\n",
        "\n",
        "### 6. Better Dataset Handling and Evaluation\n",
        "The framework provides built-in support for popular datasets such as COCO, Pascal VOC, and LVIS. It also offers easy dataset registration and standardized evaluation metrics.\n"
      ],
      "metadata": {
        "id": "dGGTtGcT6ILi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "---\n",
        "\n",
        "2.  Explain the process and importance of data annotation when working with\n",
        "Detectron2.\n",
        "\n",
        "ans :    \n",
        "\n",
        "## Process and Importance of Data Annotation in Detectron2 (5 Marks)\n",
        "\n",
        "### Process of Data Annotation in Detectron2\n",
        "\n",
        "Data annotation is the process of labeling images with ground-truth information required to train detection and segmentation models. When working with Detectron2, the annotation process generally involves the following steps:\n",
        "\n",
        "**1. Data Collection**  \n",
        "Raw images relevant to the target task such as object detection, instance segmentation, or keypoint detection are collected.\n",
        "\n",
        "**2. Annotation**  \n",
        "Annotation tools such as LabelImg, CVAT, or LabelMe are used to label objects with bounding boxes, segmentation masks, or keypoints.\n",
        "\n",
        "**3. Dataset Formatting**  \n",
        "The annotations are converted into formats supported by Detectron2, most commonly the COCO JSON format, which contains image metadata, category labels, bounding box coordinates, and segmentation masks.\n",
        "\n",
        "**4. Dataset Registration**  \n",
        "The annotated dataset is registered in Detectron2 using dataset registration APIs, allowing the framework to access the images and annotations during training and evaluation.\n",
        "\n",
        "**5. Validation and Testing**  \n",
        "The dataset is split into training, validation, and testing sets to ensure accurate performance evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### Importance of Data Annotation in Detectron2\n",
        "\n",
        "Data annotation plays a critical role in the performance of Detectron2 models:\n",
        "\n",
        "- Accurate annotations provide reliable ground truth, enabling the model to learn correct object features.\n",
        "- High-quality labels improve detection accuracy, segmentation quality, and model generalization.\n",
        "- Poor or inconsistent annotations can lead to incorrect predictions and degraded model performance.\n",
        "- Proper annotation is essential for fair evaluation using standard metrics such as mean Average Precision (mAP) and Intersection over Union (IoU).\n",
        "- Task-specific annotations (bounding boxes, masks, or keypoints) allow Detectron2 to support multiple computer vision tasks effectively.\n"
      ],
      "metadata": {
        "id": "OEoE9XBJ6XNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "3.  Describe the steps involved in training a custom object detection model\n",
        "using Detectron2.\n",
        "\n",
        "ans :    \n",
        "## Steps Involved in Training a Custom Object Detection Model Using Detectron2\n",
        "\n",
        "Training a custom object detection model using Detectron2 involves a sequence of well-defined steps, from data preparation to model evaluation.\n",
        "\n",
        "### 1. Dataset Preparation\n",
        "Collect images relevant to the detection task and annotate them using tools such as LabelImg or CVAT. The annotations are converted into a format supported by Detectron2, most commonly the COCO JSON format, which contains image details, category labels, and bounding box information.\n",
        "\n",
        "### 2. Dataset Registration\n",
        "Register the custom dataset in Detectron2 using dataset registration APIs. This step allows Detectron2 to load the images and annotations during training and evaluation.\n",
        "\n",
        "### 3. Environment Setup\n",
        "Install Detectron2 and its dependencies in the working environment (such as Google Colab). Import the required libraries and verify GPU availability for faster training.\n",
        "\n",
        "### 4. Model Configuration\n",
        "Select a pre-trained object detection model from the Detectron2 Model Zoo. Modify the configuration file to match the custom dataset, including the number of classes, dataset names, batch size, learning rate, and number of training iterations.\n",
        "\n",
        "### 5. Training the Model\n",
        "Initialize the trainer using the configured settings and start the training process. Detectron2 fine-tunes the pre-trained model on the custom dataset, updating model weights to learn task-specific features.\n",
        "\n",
        "### 6. Model Evaluation\n",
        "Evaluate the trained model using validation or test datasets. Detectron2 provides built-in evaluators that compute standard metrics such as mean Average Precision (mAP).\n",
        "\n",
        "### 7. Inference and Visualization\n",
        "Use the trained model to perform inference on new images. Visualize the predicted bounding boxes and confidence scores to assess model performance qualitatively.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "By following these steps, Detectron2 enables efficient training of custom object detection models with minimal code while maintaining high accuracy and scalability.\n"
      ],
      "metadata": {
        "id": "RKoGe35k6tqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "4. What are evaluation curves in Detectron2, and how are metrics like mAP\n",
        "and IoU interpreted?\n",
        "\n",
        "ans :    \n",
        "\n",
        "## Evaluation Curves and Metrics in Detectron2\n",
        "\n",
        "### Evaluation Curves in Detectron2\n",
        "Evaluation curves in Detectron2 are graphical representations used to analyze the performance of object detection and segmentation models. These curves help in understanding how well a model predicts objects across different confidence thresholds.\n",
        "\n",
        "Common evaluation curves include **Precision Recall (PR) curves**, which show the relationship between precision and recall at various confidence levels. Precision measures how many predicted objects are correct, while recall measures how many actual objects are successfully detected. A well-performing model produces a PR curve that remains close to the top-right corner, indicating high precision and high recall.\n",
        "\n",
        "Detectron2 also logs training and validation losses over iterations, which can be visualized as learning curves to monitor convergence and detect overfitting or underfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretation of IoU (Intersection over Union)\n",
        "Intersection over Union (IoU) is a metric used to measure the overlap between a predicted bounding box and the ground-truth bounding box. It is calculated as the area of overlap divided by the area of union of the two boxes.\n",
        "\n",
        "- IoU values range from 0 to 1.\n",
        "- A higher IoU indicates better localization accuracy.\n",
        "- Predictions are considered correct only if the IoU exceeds a predefined threshold (e.g., 0.5 or 0.75).\n",
        "\n",
        "---\n",
        "\n",
        "### Interpretation of mAP (mean Average Precision)\n",
        "Mean Average Precision (mAP) is a standard metric used to evaluate object detection performance in Detectron2. It is computed by averaging the **Average Precision (AP)** across all object classes and IoU thresholds.\n",
        "\n",
        "- AP is calculated as the area under the Precision Recall curve for a class.\n",
        "- mAP summarizes detection accuracy, localization quality, and classification performance.\n",
        "- Higher mAP values indicate better overall model performance.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "Evaluation curves and metrics such as IoU and mAP are essential for analyzing the accuracy, robustness, and generalization ability of Detectron2 models. They provide both quantitative and visual insights into detection and segmentation performance.\n"
      ],
      "metadata": {
        "id": "k8tMKfua64Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "5. Compare Detectron2 and TFOD2 in terms of features, performance, and\n",
        "ease of use.\n",
        "\n",
        "ans  :    \n",
        "\n",
        "## Comparison of Detectron2 and TensorFlow Object Detection API (TFOD2)\n",
        "\n",
        "Detectron2 and TensorFlow Object Detection API (TFOD2) are two widely used frameworks for building object detection and segmentation models. They differ in terms of features, performance, and ease of use.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Features\n",
        "\n",
        "**Detectron2**\n",
        "- Built on **PyTorch** with dynamic computation graphs.\n",
        "- Supports multiple vision tasks including **object detection, instance segmentation, semantic segmentation, panoptic segmentation, and keypoint detection**.\n",
        "- Provides state-of-the-art models such as **Faster R-CNN, Mask R-CNN, RetinaNet, and Panoptic FPN**.\n",
        "- Highly modular architecture allowing easy customization of backbones, heads, and losses.\n",
        "\n",
        "**TFOD2**\n",
        "- Built on **TensorFlow 2** and Keras.\n",
        "- Primarily focused on **object detection** tasks.\n",
        "- Supports popular models such as **SSD, Faster R-CNN, EfficientDet, and CenterNet**.\n",
        "- Integrates well with TensorFlow ecosystem tools like TensorBoard and TF Serving.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Performance\n",
        "\n",
        "**Detectron2**\n",
        "- Optimized for high performance and research-grade experiments.\n",
        "- Provides faster training and inference for complex models, especially segmentation tasks.\n",
        "- Strong benchmark results on datasets like COCO due to optimized implementations.\n",
        "\n",
        "**TFOD2**\n",
        "- Provides stable and scalable performance, especially for production deployment.\n",
        "- Efficient models such as SSD and EfficientDet perform well on resource-constrained devices.\n",
        "- Performance is highly dependent on TensorFlow graph optimizations and hardware accelerators.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Ease of Use\n",
        "\n",
        "**Detectron2**\n",
        "- Easier to debug and experiment due to PyTorch’s dynamic graph support.\n",
        "- Clean and well-documented codebase, but requires familiarity with PyTorch.\n",
        "- Dataset registration and customization are straightforward for research use.\n",
        "\n",
        "**TFOD2**\n",
        "- More complex initial setup and configuration process.\n",
        "- Requires working with configuration files and TensorFlow pipelines.\n",
        "- Better suited for large-scale production environments once configured.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Aspect | Detectron2 | TFOD2 |\n",
        "|------|-----------|-------|\n",
        "| Framework | PyTorch | TensorFlow 2 |\n",
        "| Supported Tasks | Detection, Segmentation, Keypoints | Mainly Detection |\n",
        "| Customization | High | Moderate |\n",
        "| Performance | High (research-focused) | Stable (production-focused) |\n",
        "| Ease of Use | Easier for research | Steeper learning curve |\n",
        "| Production Deployment | Moderate | Strong |\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "Detectron2 is preferred for research and experimentation due to its flexibility, modularity, and strong performance in advanced vision tasks. TFOD2 is more suitable for production environments that require tight integration with the TensorFlow ecosystem and scalable deployment solutions.\n",
        "\n"
      ],
      "metadata": {
        "id": "qkW7Trym7TCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "6. Write Python code to install Detectron2 and verify the installation.\n",
        "\n"
      ],
      "metadata": {
        "id": "yn00yGHd72Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install opencv-python pycocotools\n"
      ],
      "metadata": {
        "id": "wV3wQm6I_aK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install detectron2 -f \\\n",
        "https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch2.0/index.html\n"
      ],
      "metadata": {
        "id": "U8grruaGOCaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import detectron2\n",
        "print(\"Detectron2 imported successfully\")\n",
        "\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "print(\"Detectron2 is fully working (CPU mode)\")\n"
      ],
      "metadata": {
        "id": "31bpzp1YOFtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Annotate a dataset using any tool of your choice and convert the\n",
        "annotations to COCO format for Detectron2.\n"
      ],
      "metadata": {
        "id": "011qX8pbOGVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install detectron2\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "# Install other requirements\n",
        "!pip install pyyaml==5.1\n",
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
      ],
      "metadata": {
        "id": "EsYCaFFDOfIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "# --- STEP 1: Download Sample Data (Skip if using your own) ---\n",
        "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n",
        "!unzip -q balloon_dataset.zip\n",
        "\n",
        "# --- STEP 2: Register the Dataset ---\n",
        "# Format: register_coco_instances(name, metadata, json_file, image_root)\n",
        "# NOTE: If your JSON is from CVAT/Roboflow, point to those specific paths.\n",
        "register_coco_instances(\"my_dataset_train\", {}, \"balloon/train/via_region_data.json\", \"balloon/train/\")\n",
        "register_coco_instances(\"my_dataset_val\", {}, \"balloon/val/via_region_data.json\", \"balloon/val/\")\n",
        "\n",
        "# --- STEP 3: Verify Registration ---\n",
        "dataset_dicts = DatasetCatalog.get(\"my_dataset_train\")\n",
        "balloon_metadata = MetadataCatalog.get(\"my_dataset_train\")\n",
        "\n",
        "# Visualize 3 random samples to ensure annotations are correct\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "9HVNNJZXOfvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "cfg = get_cfg()\n",
        "# Load a model config (Instance Segmentation in this case)\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 300    # 300 iterations is enough for a tiny dataset; adjust for yours\n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # ONLY 1 class (balloon). Change this to match your dataset!\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "S6wxR3GaOi7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference should use the config with newly trained weights\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "dataset_dicts = DatasetCatalog.get(\"my_dataset_val\")\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=balloon_metadata,\n",
        "                   scale=0.5,\n",
        "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. Useful to confirm segmentation\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "metadata": {
        "id": "JTovZFPMOlTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a script to download pretrained weights and configure paths for\n",
        "training in Detectron2.\n"
      ],
      "metadata": {
        "id": "uvSMGznxOnlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup_detectron2_training.py\n",
        "\n",
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "def main():\n",
        "    # ---------- 1. Configure dataset paths & register ----------\n",
        "    DATASET_ROOT = \"/path/to/your/dataset\"          # <- change this\n",
        "    TRAIN_JSON   = os.path.join(DATASET_ROOT, \"annotations/train.json\")\n",
        "    VAL_JSON     = os.path.join(DATASET_ROOT, \"annotations/val.json\")\n",
        "    TRAIN_IMG    = os.path.join(DATASET_ROOT, \"train\")\n",
        "    VAL_IMG      = os.path.join(DATASET_ROOT, \"val\")\n",
        "\n",
        "    register_coco_instances(\"my_dataset_train\", {}, TRAIN_JSON, TRAIN_IMG)\n",
        "    register_coco_instances(\"my_dataset_val\",   {}, VAL_JSON,   VAL_IMG)\n",
        "\n",
        "    # ---------- 2. Build config and download pretrained weights ----------\n",
        "    cfg = get_cfg()\n",
        "\n",
        "    # Base config from Detectron2 model zoo\n",
        "    config_file = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(config_file))\n",
        "\n",
        "    # This URL points to pretrained COCO weights and will be auto-downloaded\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_file)\n",
        "\n",
        "    # ---------- 3. Set training-related paths and options ----------\n",
        "    cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "    cfg.DATASETS.TEST  = (\"my_dataset_val\",)\n",
        "\n",
        "    cfg.DATALOADER.NUM_WORKERS = 4\n",
        "    cfg.SOLVER.IMS_PER_BATCH   = 2\n",
        "    cfg.SOLVER.BASE_LR         = 0.00025\n",
        "    cfg.SOLVER.MAX_ITER        = 10000\n",
        "\n",
        "    # Number of classes in your dataset (exclude background)\n",
        "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "\n",
        "    # Output directory for logs and checkpoints\n",
        "    cfg.OUTPUT_DIR = \"./output/mask_rcnn_training\"\n",
        "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # ---------- 4. Start training ----------\n",
        "    trainer = DefaultTrainer(cfg)\n",
        "    trainer.resume_or_load(resume=False)\n",
        "    trainer.train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "1sRR83d9PwX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Show the steps and code to run inference using a trained Detectron2\n",
        "model on a new image."
      ],
      "metadata": {
        "id": "_Ny9erc7PzZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "\n",
        "def setup_cfg(trained_output_dir, score_thresh=0.5, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Load config and trained weights from an output directory.\n",
        "    \"\"\"\n",
        "    cfg = get_cfg()\n",
        "\n",
        "    # Load the config you used for training\n",
        "    config_path = os.path.join(trained_output_dir, \"config.yaml\")\n",
        "    cfg.merge_from_file(config_path)\n",
        "\n",
        "    # Use the trained weights\n",
        "    cfg.MODEL.WEIGHTS = os.path.join(trained_output_dir, \"model_final.pth\")\n",
        "\n",
        "    # Set score threshold for this model\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = score_thresh\n",
        "\n",
        "    # Choose device: \"cuda\" or \"cpu\"\n",
        "    cfg.MODEL.DEVICE = device\n",
        "\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def run_inference(\n",
        "    trained_output_dir,\n",
        "    input_image_path,\n",
        "    output_image_path=None,\n",
        "    metadata_dataset_name=\"my_dataset_train\",\n",
        "):\n",
        "    # 1. Build config and predictor\n",
        "    cfg = setup_cfg(trained_output_dir)\n",
        "    predictor = DefaultPredictor(cfg)\n",
        "\n",
        "    # 2. Read image\n",
        "    image = cv2.imread(input_image_path)\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(f\"Could not read image: {input_image_path}\")\n",
        "\n",
        "    # 3. Run prediction\n",
        "    outputs = predictor(image)\n",
        "    # outputs[\"instances\"] has fields: pred_boxes, scores, pred_classes, (and masks for instance seg)\n",
        "\n",
        "    # 4. Visualization\n",
        "    metadata = MetadataCatalog.get(metadata_dataset_name)\n",
        "    v = Visualizer(\n",
        "        image[:, :, ::-1],  # convert BGR to RGB for Vis\n",
        "        metadata=metadata,\n",
        "        scale=1.0,\n",
        "        instance_mode=ColorMode.IMAGE_BW,  # background grayscale\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "    result_image = out.get_image()[:, :, ::-1]  # back to BGR for cv2\n",
        "\n",
        "    # 5. Save or display\n",
        "    if output_image_path is not None:\n",
        "        os.makedirs(os.path.dirname(output_image_path), exist_ok=True)\n",
        "        cv2.imwrite(output_image_path, result_image)\n",
        "        print(f\"Saved result to {output_image_path}\")\n",
        "    else:\n",
        "        # Show on screen (press a key to close)\n",
        "        cv2.imshow(\"Result\", result_image)\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    TRAINED_OUTPUT_DIR = \"./output/mask_rcnn_training\"  # where config.yaml & model_final.pth are\n",
        "    INPUT_IMAGE = \"path/to/your/image.jpg\"\n",
        "    OUTPUT_IMAGE = \"output/inference_result.jpg\"  # or None to just display\n",
        "\n",
        "    run_inference(\n",
        "        trained_output_dir=TRAINED_OUTPUT_DIR,\n",
        "        input_image_path=INPUT_IMAGE,\n",
        "        output_image_path=OUTPUT_IMAGE,\n",
        "        metadata_dataset_name=\"my_dataset_train\",  # same as training dataset name\n",
        "    )"
      ],
      "metadata": {
        "id": "1qJJcdJ9P7Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are assigned to build a wildlife monitoring system to detect and track\n",
        "different animal species in a forest using Detectron2. Describe the end-to-end pipeline\n",
        "from data collection to deploying the model, and how you would handle challenges like\n",
        "occlusion or nighttime detection.\n"
      ],
      "metadata": {
        "id": "VL-PmB_LP-l7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Data collection**  \n",
        "   - Install camera traps in the forest; record videos/images in different seasons, locations, and times (day + night, RGB + IR/thermal).\n",
        "\n",
        "2. **Annotation & dataset**  \n",
        "   - Extract frames, annotate animals with bounding boxes and species labels using a tool like CVAT.  \n",
        "   - Export in COCO format and split into train/val/test.\n",
        "\n",
        "3. **Setup in Google Colab (Detectron2)**  \n",
        "   - Install Detectron2, mount Google Drive, and point to your `images/` and `annotations/` folders.  \n",
        "   - Register datasets with `register_coco_instances(\"wildlife_train\", ...)`.\n",
        "\n",
        "4. **Model training with Detectron2**  \n",
        "   - Start from a COCO‑pretrained model (e.g., Mask R‑CNN R50‑FPN).  \n",
        "   - Set `NUM_CLASSES` to the number of species, adjust learning rate/iterations, and train with `DefaultTrainer`.  \n",
        "   - Use basic augmentations (flip, scale, brightness) to improve robustness.\n",
        "\n",
        "5. **Inference + tracking**  \n",
        "   - Use `DefaultPredictor` to run the trained model on each frame and get species + bounding boxes.  \n",
        "   - Feed detections into a multi‑object tracker (e.g., DeepSORT/ByteTrack) to assign track IDs and follow each animal across frames.\n",
        "\n",
        "6. **Deployment**  \n",
        "   - Edge: run the model + tracker on a Jetson or similar device near the camera and send only detection summaries.  \n",
        "   - Server: stream video to a central machine that runs inference, tracking, and stores results (counts, tracks, timestamps).\n",
        "\n",
        "7. **Handling occlusion**  \n",
        "   - Include partially visible animals in training data; use augmentations like random erasing/CutOut.  \n",
        "   - Use a tracker that tolerates a few missed frames so animals aren’t “lost” when briefly behind trees or other animals.\n",
        "\n",
        "8. **Handling nighttime detection**  \n",
        "   - Use IR or thermal cameras; include nighttime images in training.  \n",
        "   - Optionally train separate day and night models, or one mixed model with low‑light augmentations (brightness/contrast changes, noise).  \n",
        "   - Simple preprocessing (e.g., contrast enhancement) can help for very dark images."
      ],
      "metadata": {
        "id": "MlBjrMbiQmcx"
      }
    }
  ]
}